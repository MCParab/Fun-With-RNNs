{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrrent Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "# Loading the training data \n",
    "data = open('kafka.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('Data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Encode/Decode Char/Vector \n",
    "\n",
    "Neural Networks operate on vectors (a vector is an array of float). So we need a way to \n",
    "encode and decode a char as a vector.\n",
    "\n",
    "We will count the number of unique chars (vocab_size). That will be the size of the vector.\n",
    "The vector contains only zero except for the position of the char whereas the value is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 0, 'e': 1, 'q': 59, ')': 70, 'b': 3, 'L': 17, 'z': 5, '.': 49, '8': 6, 'j': 8, '/': 44, 'P': 9, 'H': 10, 'x': 58, 'N': 11, '4': 46, 'p': 12, 'y': 13, 'B': 14, '9': 19, 'k': 16, 'c': 2, 'U': 18, 'X': 22, 'o': 20, 'K': 21, 'f': 23, '$': 25, 'g': 26, ',': 27, 'l': 28, 's': 30, '7': 74, 'v': 60, '\"': 31, 'E': 4, 'h': 32, '!': 34, 'M': 39, '0': 41, ' ': 37, '3': 42, 'R': 61, 'O': 43, 'r': 7, ';': 45, '?': 47, 'Y': 38, 'I': 51, '%': 24, '@': 52, 'T': 33, 'C': 54, \"'\": 50, '6': 56, '\\n': 57, 'n': 15, 'm': 36, 'W': 62, '2': 63, 'i': 64, 'รง': 40, 'V': 65, 'a': 66, 'A': 53, ':': 68, 't': 69, 'G': 48, 'F': 67, 'w': 71, 'J': 55, 'S': 35, 'u': 72, '*': 73, '5': 75, 'D': 29, '-': 76, 'Q': 77, '1': 78, '(': 79}\n",
      "{0: 'd', 1: 'e', 2: 'c', 3: 'b', 4: 'E', 5: 'z', 6: '8', 7: 'r', 8: 'j', 9: 'P', 10: 'H', 11: 'N', 12: 'p', 13: 'y', 14: 'B', 15: 'n', 16: 'k', 17: 'L', 18: 'U', 19: '9', 20: 'o', 21: 'K', 22: 'X', 23: 'f', 24: '%', 25: '$', 26: 'g', 27: ',', 28: 'l', 29: 'D', 30: 's', 31: '\"', 32: 'h', 33: 'T', 34: '!', 35: 'S', 36: 'm', 37: ' ', 38: 'Y', 39: 'M', 40: 'รง', 41: '0', 42: '3', 43: 'O', 44: '/', 45: ';', 46: '4', 47: '?', 48: 'G', 49: '.', 50: \"'\", 51: 'I', 52: '@', 53: 'A', 54: 'C', 55: 'J', 56: '6', 57: '\\n', 58: 'x', 59: 'q', 60: 'v', 61: 'R', 62: 'W', 63: '2', 64: 'i', 65: 'V', 66: 'a', 67: 'F', 68: ':', 69: 't', 70: ')', 71: 'w', 72: 'u', 73: '*', 74: '7', 75: '5', 76: '-', 77: 'Q', 78: '1', 79: '('}\n"
     ]
    }
   ],
   "source": [
    "# Calculating the vocab_size .......\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Creating 2 dictionary to encode and decode a char to an int \n",
    "# We create a vector from a char like this :\n",
    "# the dictionary defined above allows us to create a vector of size 61 instead of 256\n",
    "# Here an example of char 'a'\n",
    "# The vector contains only zeros, except at position char_to_ix['a'] where we put 1\n",
    "import numpy as np\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture \n",
    "The neural network is made of 3 layers :\n",
    "1. an input layer \n",
    "2. an hidden layer \n",
    "3. output layer \n",
    "\n",
    "All layers are fully connected to one another : each node of a layer are connected to all nodes of the next layer. The hidden layer is connected to the output and to itself : the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (hyperparametes) we also define the sequence length and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)* 0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)* 0.01 # hidden to hidden \n",
    "Why = np.random.randn(vocab_size, hidden_size)* 0.001 # hidden to output\n",
    "bh = np.zeros((hidden_size,1))\n",
    "by = np.zeros((hidden_size,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "It is a value that describe how giid is our model. The smaller the loss, better our model is. During training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients.\n",
    "\n",
    "* It performs a forward pass, calculate the next char given a char from the training set.\n",
    "* It calculate the loss by computing the predicted char to the target char.\n",
    "* It calculate the backward pass to calculate the gradients.\n",
    "\n",
    "The function take as input :\n",
    " * a list of input char \n",
    " * a list of target char \n",
    " * and the previous hidden state \n",
    " \n",
    "This function outputs : \n",
    " * The loss \n",
    " * The gradient for each parameters between layers \n",
    " * The last hidden state \n",
    " \n",
    "## Forward Pass : \n",
    "\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the \n",
    "next char given a char from the training set.\n",
    "xs[t] is the vector that encode the char at position t ps[t] is the probabilities for next char\n",
    "\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1] +bh)\n",
    "ys[t] = np.dot(Why, hs[t]) + by \n",
    "ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "\n",
    "## Backward Pass \n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters. This is possible but would be time consuming. There is a technics to calculate all gradients for all the parameters at once : the backdrop propagation.\n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.\n",
    "#### Goal is to calculate gradients for the forward formula.\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh \n",
    "ys = hs*Why + by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lossFun(inputs, targets, hprev):\n",
    "        \"\"\"\n",
    "        inputs, targets are both list of integers.\n",
    "        hprev is Hx1 array of initial hidden state \n",
    "        returns the loss, gradients on model parameters, and last hidden state \n",
    "        \"\"\"\n",
    "        # store out inputs, hidden states, outputs, and probability values \n",
    "        xs, hs, ys, ps, = {}, {}, {}, {} # Empty dicts \n",
    "        \n",
    "        hs[-1] = np.copy(hprev)\n",
    "        # initial loss as 0 \n",
    "        loss = 0\n",
    "        \n",
    "        # forward pass \n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((vocab_size, 1)) # encode on 1-of-k representation (we place a 0 vector as t-th input)\n",
    "            xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to set the correct \n",
    "            hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state \n",
    "            ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars \n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars \n",
    "            loss += -np.log(ps[t][targets[t], 0])   # Softmax (cross-entropy loss)\n",
    "        # Backward pass : compute gradients going backwards \n",
    "        # initialize vectors for gradient values for each set of weights \n",
    "        dwxh, dwhh, dwhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            # ouput probabilities \n",
    "            dy = np.copy(ps[t])\n",
    "            # derive our first gradient \n",
    "            dy[targets[t]] -= 1 # backprop into y \n",
    "            # compute output gradient - output times hidden states transpose \n",
    "            # When we apply the transpose wight matrix \n",
    "            # We can think intuitively of this as moving the error backward \n",
    "            # through the network, giving us some sort of measure of the error \n",
    "            # at the output of the lth layer \n",
    "            # output gradient \n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            # Derivative of output bias \n",
    "            dby += dy \n",
    "            # Backpropogate \n",
    "            dh = np.dot(Why.T, dy) + dhnext \n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh \n",
    "            dbh += dhraw \n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out = dparam)\n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a sentence from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction, one full forward pass \n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step \n",
    "    n is how many characters to predict \n",
    "    \"\"\"\n",
    "    \n",
    "    # create vector \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # customiize it for our seed char \n",
    "    x[seed_ix] = 1\n",
    "    # list to store generated chars \n",
    "    ixes = []\n",
    "    # for as many characters as we want to generate \n",
    "    for t in range(n):\n",
    "        h =  np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        # compute output (unormalised)\n",
    "        y = np.dot(Why, h) + by \n",
    "        ## probabilities for next chars \n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # pick one with the highest probabilities\n",
    "        ix = np.random.choice(range(vocab_size), p = p.ravel())\n",
    "        # create a vector \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        # customize it for the predicted char \n",
    "        x[ix] = 1\n",
    "        # add it to the list \n",
    "        ixes.append(ix)\n",
    "        \n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "        print ('-----\\n %s \\n------' % (txt, ))\n",
    "        \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    # predict the 200 characters given 'a'\n",
    "    sample(hprev, char_to_ix['a'], 200)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
